{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7a38e6-341d-45b1-b977-6e59bbb712b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "API_KEY=\"hack-with-upstage-solar-0420\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3f777ef-787c-4186-b1d9-f4937dedc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this system prompt will be used globally on the chat session.\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are a work-management assistant that assigns work-items to available employees. You will be provided data in the following format:\n",
    "    # AVAILABLE WORK ITEMS\n",
    "    ## work_item_id\t\\t work_description\t\\t work_type\n",
    "    \n",
    "    # AVAILABLE EMPLOYEES\n",
    "    ## employee_id \\t work_type_preference\n",
    "\n",
    "    # INSTRUCTIONS:\n",
    "    ## 1. YOU NEED TO MAKE SURE THAT EMPLOYEES ARE ASSIGNED WORK ITEMS BASED ON EMPLOYEE-PREFERENCES.\n",
    "    ## FOR EXAMPLE IF A EMPLOYEE-PREFERENCE IS 1. Front end, 2. Back end, assign them Front end tasks if possible, otherwise Back end tasks.\n",
    "    ## 2. TRY TO ASSIGN 1 WI TO EACH EMPLOYEE, IF POSSIBLE\n",
    "    ## 3. OUTPUT THE ASSIGNMENT AS MARKDOWN, IN THE FOLLOWING FORMAT:\n",
    "    ## WI ID - EMPLOYEE ID\n",
    "    ## For example:\n",
    "    ## WI-A - Employee X\n",
    "    ## WI-B - Employee Y\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73e7ca9f-7490-4875-aaf2-80ad864bd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"solar-1-mini-chat\"\n",
    " \n",
    "# # Keep chat history in a list.\n",
    "# chat_history = []\n",
    " \n",
    "# # Set a limit for the chat history to manage the token count.\n",
    "# history_size = 10\n",
    " \n",
    "# while True:\n",
    "#     # Step 1: Get user input.\n",
    "#     user_prompt = input(\"User: \")\n",
    "#     chat_history.append({\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": user_prompt\n",
    "#     })\n",
    " \n",
    "#     # Step 2: Use the chat history to generate the chatbot's response.\n",
    "#     messages = [system_prompt] + chat_history\n",
    "#     stream = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#         stream=True,\n",
    "#     )\n",
    " \n",
    "#     # Step 3: Output the Solar response chunks.\n",
    "#     print(\"SOLAR: \", end=\"\")\n",
    "#     solar_response = \"\"\n",
    "#     for chunk in stream:\n",
    "#         response_content = chunk.choices[0].delta.content\n",
    "#         if response_content is not None:\n",
    "#             solar_response += response_content\n",
    "#             print(response_content, end=\"\")\n",
    "#     print() # Finish response output.\n",
    " \n",
    "#     # Step 4: Append the Solar response to the chat history.\n",
    "#     chat_history.append({\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"content\": solar_response\n",
    "#     })\n",
    " \n",
    "#     # Step 5: Ensure the chat history doesn't exceed the size limit.\n",
    "#     chat_history = chat_history[:history_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80dafc6-73d6-48a8-94e6-f225cac1b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR: Here is a sample assignment in Markdown format based on the given task and employee preferences:\n",
      "\n",
      "WI-A - Employee 1\n",
      "WI-B - Employee 2\n"
     ]
    }
   ],
   "source": [
    "model = \"solar-1-mini-chat\"\n",
    " \n",
    "# Keep chat history in a list.\n",
    "chat_history = []\n",
    " \n",
    "# Set a limit for the chat history to manage the token count.\n",
    "history_size = 10\n",
    " \n",
    "# Step 1: Get user input.\n",
    "# user_prompt = input(\"User: \")\n",
    "chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "    These are a set of tasks that need to be assigned:\n",
    "    Task 1: [Backend] Design a backend system for credit card payment processing for JP Morgan.\n",
    "    Task 2: [Frontend] Design a website for PayPal to accept payments from users.\n",
    "    \n",
    "    These are a set of employees available and their preferences, in order:\n",
    "    Employee 1: Frontend, Backend \n",
    "    Employee 2: Backend, Frontend\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# Step 2: Use the chat history to generate the chatbot's response.\n",
    "messages = [system_prompt] + chat_history\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Step 3: Output the Solar response chunks.\n",
    "print(\"SOLAR: \", end=\"\")\n",
    "solar_response = \"\"\n",
    "for chunk in stream:\n",
    "    response_content = chunk.choices[0].delta.content\n",
    "    if response_content is not None:\n",
    "        solar_response += response_content\n",
    "        print(response_content, end=\"\")\n",
    "print() # Finish response output.\n",
    "\n",
    "# Step 4: Append the Solar response to the chat history.\n",
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": solar_response\n",
    "})\n",
    "\n",
    "# Step 5: Ensure the chat history doesn't exceed the size limit.\n",
    "chat_history = chat_history[:history_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0ef52f9-d537-4992-a52f-2dbdb35c7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR: Based on the given preferences, here's the work item assignment:\n",
      "\n",
      "Front-end:\n",
      "WI-31245 - EMP-45\n",
      "WI-48392 - EMP-4\n",
      "\n",
      "Back-end:\n",
      "WI-25834 - EMP-27\n",
      "WI-37429 - EMP-3\n",
      "\n",
      "Testing:\n",
      "WI-23745 - EMP-31\n",
      "WI-48236 - EMP-47\n",
      "\n",
      "ML:\n",
      "WI-35482 - EMP-2\n",
      "WI-28134 - EMP-24\n",
      "\n",
      "Note: Since there are more employees available for Back-end work, it wasn't possible to assign 1 work item to each employee.\n"
     ]
    }
   ],
   "source": [
    "model = \"solar-1-mini-chat\"\n",
    " \n",
    "# Keep chat history in a list.\n",
    "chat_history = []\n",
    " \n",
    "# Set a limit for the chat history to manage the token count.\n",
    "history_size = 10\n",
    " \n",
    "# Step 1: Get user input.\n",
    "# user_prompt = input(\"User: \")\n",
    "chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "    These are a set of tasks that need to be assigned:\n",
    "work_item_id \\t work_description \\t work_type\t\n",
    "WI-31245 \\t Integrate responsive navigation bar using CSS Flexbox and JavaScript for interactivity.\t\\t Front-end\n",
    "WI-48392 \\t Develop a React component for a live search feature that queries an API as the user types. \\t Front-end\n",
    "WI-25834 \\t Set up a RESTful API with authentication and authorization mechanisms. \\t Back-end\n",
    "WI-37429 \\t Implement database schema migrations to add new features to the backend system. \\t Back-end\n",
    "WI-23745 \\t Write unit tests for JavaScript functions using a testing framework like Jest. \\t Testing\n",
    "WI-48236 \\t Conduct cross-browser testing to ensure compatibility across major browsers. \\t Testing\n",
    "WI-35482 \\t Train a machine learning model to classify images using TensorFlow or PyTorch. \\t ML\n",
    "WI-28134 \\t Implement a recommendation system using collaborative filtering techniques.\t\\t ML\n",
    "    \n",
    "    These are a set of employees available and their preferences, in order:\n",
    "employee_id\t\\t work_type_preference\n",
    "EMP-4 \\t Front-end\n",
    "EMP-3 \\t Back-end\n",
    "EMP-24 \\t ML\n",
    "EMP-31 \\t Testing\n",
    "EMP-27 \\t Back-end\n",
    "EMP-2 \\t ML\n",
    "EMP-45 \\t Front-end\n",
    "EMP-47 \\t Testing\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# Step 2: Use the chat history to generate the chatbot's response.\n",
    "messages = [system_prompt] + chat_history\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Step 3: Output the Solar response chunks.\n",
    "print(\"SOLAR: \", end=\"\")\n",
    "solar_response = \"\"\n",
    "for chunk in stream:\n",
    "    response_content = chunk.choices[0].delta.content\n",
    "    if response_content is not None:\n",
    "        solar_response += response_content\n",
    "        print(response_content, end=\"\")\n",
    "print() # Finish response output.\n",
    "\n",
    "# Step 4: Append the Solar response to the chat history.\n",
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": solar_response\n",
    "})\n",
    "\n",
    "# Step 5: Ensure the chat history doesn't exceed the size limit.\n",
    "chat_history = chat_history[:history_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ecb0b-aee2-4a9c-9d43-06c2d82dfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pd.read_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
